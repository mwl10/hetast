{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe134d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.chdir('/content')\n",
    "if os.path.isdir('./hetvae'):\n",
    "  shutil.rmtree('./hetvae/')\n",
    "! git clone https://github.com/mwl10/hetvae\n",
    "os.chdir('/content/hetvae')\n",
    "! git checkout optuna\n",
    "! pip install -r requirements.txt\n",
    "os.chdir('/content/hetvae/src')\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch.optim as optim\n",
    "import models\n",
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import vae_models\n",
    "importlib.reload(vae_models)\n",
    "importlib.reload(models)\n",
    "importlib.reload(utils)\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "#del sys.modules['dataset']\n",
    "from dataset import DataSet\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# AGN Preprocessing, we have a DF w/ OBJECT , FILTER, MJD, FLUX, ERROR\n",
    "#-------------------------------\n",
    "agn_file = '/content/hetvae/data/AGN_1H2106-099/1H2106-099_latest_lcs_pyroa.csv'\n",
    "agn_df = pd.read_csv(agn_file)\n",
    "# handle formatting for new AGN\n",
    "lcs = []\n",
    "for lc_df in agn_df.groupby('Filter'):\n",
    "    lc = lc_df[1][['MJD', 'Flux', 'Error']].to_numpy()\n",
    "    lcs.append(lc)\n",
    "\n",
    "AGN_1H2106 = DataSet()\n",
    "AGN_1H2106.dataset = lcs # skip add files part\n",
    "AGN_1H2106 = AGN_1H2106.handle_dups().resample_dataset(num_samples=1).normalize(normalize_y_by='individual') \\\n",
    "  .set_union_x().zero_fill().make_masks(frac=0.5)\n",
    "\n",
    "print(\n",
    "    AGN_1H2106.union_x.shape,\n",
    "    AGN_1H2106.dataset.shape,\n",
    "    AGN_1H2106.subsampled_mask.shape,\n",
    "    AGN_1H2106.recon_mask.shape\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3120f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------\n",
    "# PREPROCESSING, from edelson data \n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "lc_files = glob('/content/hetvae/data/EDELSON/*')[:4]\n",
    "\n",
    "# hypers: \n",
    "Edelson = DataSet().add_files(lc_files).files_to_numpy().handle_dups().resample_dataset(num_samples=1).normalize(normalize_y_by='individual') \\\n",
    "        .set_union_x().zero_fill().make_masks(frac=0.5)\n",
    "\n",
    "print(\n",
    "    Edelson.union_x.shape,\n",
    "    Edelson.dataset.shape,\n",
    "    Edelson.subsampled_mask.shape,\n",
    "    Edelson.recon_mask.shape\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ad704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num heads?\n",
    "args = Namespace(batch_size=8, bound_variance=True, const_var=False, dataset='toy', dropout=0.0, \n",
    "                 elbo_weight=1.0, embed_time=128, enc_num_heads=1, intensity=True, k_iwae=1, kl_annealing=False, \n",
    "                 kl_zero=False, latent_dim=64, lr=0.000001, mixing='concat', mse_weight=2.0, n=256, net='hetvae', \n",
    "                 niters=1000, norm=True, normalize_input='znorm', num_ref_points=16, rec_hidden=16, recon_loss=False, \n",
    "                 sample_tp=0.5, save=True, seed=0, shuffle=True, std=0.1, var_per_dim=False, width=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4088220",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "FILES = glob('/content/hetvae/data/EDELSON/*')[:4]\n",
    "DIM = 1\n",
    "# definitly want a decaying error rate...\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[], gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aebfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LightCurves = np.concatenate((Edelson.dataset, Edelson.subsampled_mask[:,:,np.newaxis], Edelson.recon_mask[:,:,np.newaxis]), axis=-1) # format the masks for the model \n",
    "print(LightCurves.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fa40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, valid = np.split(LightCurves, [int(np.floor(0.8*len(LightCurves)))])# shuffle?\n",
    "train_loader = torch.utils.data.DataLoader(training, batch_size=args.batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=args.batch_size)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.load_network(args, DIM, torch.Tensor(Edelson.union_x)) # , device=\"cuda\"\n",
    "params = list(net.parameters())\n",
    "optimizer = optim.Adam(params, lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer,epoch, train_loader, args, device=\"cuda\"):\n",
    "      \n",
    "      train_loss = 0.\n",
    "      train_n = 0.\n",
    "      avg_loglik, avg_kl, mse, mae = 0., 0., 0., 0.\n",
    "      for i, train_batch in enumerate(train_loader):\n",
    "          batch_len = train_batch.shape[0] \n",
    "          train_batch = train_batch.to(device)\n",
    "          x = train_batch[:,:,0]\n",
    "          y = train_batch[:,:,1:2]\n",
    "          \n",
    "          subsampled_mask = train_batch[:,:,3:4]\n",
    "          recon_mask = train_batch[:,:,4:]\n",
    "          error_bars = train_batch[:,:,2:3]\n",
    "          seqlen = train_batch.size(1) \n",
    "          # subsampled flux values and their corresponding masks....\n",
    "          context_y = torch.cat((\n",
    "              y * subsampled_mask, subsampled_mask\n",
    "          ), -1) \n",
    "          recon_context_y = torch.cat((            # flux values with only recon_mask values showing\n",
    "                  y * recon_mask, recon_mask\n",
    "              ), -1)\n",
    "          \n",
    "          masked_error_bars = error_bars * subsampled_mask \n",
    "\n",
    "\n",
    "    # #   def compute_unsupervised_loss(self, context_x, context_y, target_x, target_y, num_samples=1, beta=1):\n",
    "          loss_info = net.compute_unsupervised_loss(\n",
    "              x, # context_x, times\n",
    "              context_y,             # context_y\n",
    "              x, # target_x, same times? \n",
    "              recon_context_y,\n",
    "              num_samples=args.k_iwae, # 1? \n",
    "              beta=1,\n",
    "              # optional, will be zero if not set\n",
    "              error_bars = 0.   # masked_error_bars\n",
    "\n",
    "          )\n",
    "          optimizer.zero_grad()\n",
    "          loss_info.composite_loss.backward()\n",
    "          optimizer.step()\n",
    "          #scheduler.step()\n",
    "          train_loss += loss_info.composite_loss.item() * batch_len\n",
    "          avg_loglik += loss_info.loglik * batch_len\n",
    "          avg_kl += loss_info.kl * batch_len\n",
    "          mse += loss_info.mse * batch_len\n",
    "          mae += loss_info.mae * batch_len\n",
    "          train_n += batch_len\n",
    "      \n",
    "      \n",
    "      if epoch % 100 == 0:\n",
    "          print(\n",
    "              'Iter: {}, train loss: {:.4f}, avg nll: {:.4f}, avg kl: {:.4f}, '\n",
    "              'mse: {:.6f}, mae: {:.6f}'.format(\n",
    "                  epoch,\n",
    "                  train_loss / train_n,\n",
    "                  -avg_loglik / train_n,\n",
    "                  avg_kl / train_n,\n",
    "                  mse / train_n,\n",
    "                  mae / train_n\n",
    "              )\n",
    "          )\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a537d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(201, 1000+1):#args.niters + 1):\n",
    "    train(net, optimizer, epoch, train_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    print(g['lr'])\n",
    "    g['lr'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, train_loader, device=\"cuda\"):\n",
    "    train_loss = 0.\n",
    "    train_n = 0.\n",
    "    avg_loglik, mse, mae = 0,0,0\n",
    "    mean_mae, mean_mse = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, train_batch in enumerate(train_loader):\n",
    "            batch_len = train_batch.shape[0] \n",
    "            train_batch = train_batch.to(device)\n",
    "            x = train_batch[:,:,0]\n",
    "            y = train_batch[:,:,1:2]\n",
    "            subsampled_mask = train_batch[:,:,3:4]\n",
    "            recon_mask = train_batch[:,:,4:]\n",
    "            error_bars = train_batch[:,:,2:3]\n",
    "            seqlen = train_batch.size(1) \n",
    "            # subsampled flux values and their corresponding masks....\n",
    "            context_y = torch.cat((\n",
    "                y * subsampled_mask, subsampled_mask\n",
    "            ), -1) \n",
    "            recon_context_y = torch.cat((            # flux values with only recon_mask values showing\n",
    "                    y * recon_mask, recon_mask\n",
    "                ), -1)\n",
    "            \n",
    "            masked_error_bars = error_bars * subsampled_mask \n",
    "\n",
    "\n",
    "      # #   def compute_unsupervised_loss(self, context_x, context_y, target_x, target_y, num_samples=1, beta=1):\n",
    "            loss_info = net.compute_unsupervised_loss(\n",
    "                x, # context_x, times\n",
    "                context_y,             # context_y\n",
    "                x, # target_x, same times? \n",
    "                recon_context_y,\n",
    "                num_samples=1, #???\n",
    "                beta=1,\n",
    "                # optional, will be zero if not set\n",
    "                error_bars = 0.   # masked_error_bars\n",
    "\n",
    "            )\n",
    "\n",
    "            num_context_points = recon_mask.sum().item()\n",
    "            mse += loss_info.mse * num_context_points\n",
    "            mae += loss_info.mae * num_context_points\n",
    "            mean_mse += loss_info.mean_mse * num_context_points\n",
    "            mean_mae += loss_info.mean_mae * num_context_points\n",
    "            avg_loglik += loss_info.mogloglik * num_context_points\n",
    "            train_n += num_context_points\n",
    "    print(-avg_loglik / train_n, \"NONE???\")\n",
    "    print(\n",
    "    'nll: {:.4f}, mse: {:.4f}, mae: {:.4f}, '\n",
    "    'mean_mse: {:.4f}, mean_mae: {:.4f}'.format(\n",
    "        - avg_loglik / train_n,\n",
    "        mse / train_n,\n",
    "        mae / train_n,\n",
    "        mean_mse / train_n,\n",
    "        mean_mae / train_n\n",
    "    ))\n",
    "      \n",
    "    \n",
    "    return float(- avg_loglik / train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation for one light curve w/ increasing number of points\n",
    "\n",
    "def viz_per_example(lc, net, k_iwae=40, n_max=10, fracs=[0.2,0.2,0.5]): \n",
    "    pred_mean, pred_std = [], []\n",
    "    masks = []\n",
    "    targets = []\n",
    "    tp =[]\n",
    "    lc = lc[np.newaxis, :,:]\n",
    "\n",
    "    np.random.seed(0)\n",
    "    with torch.no_grad():\n",
    "        for frac in fracs: # \n",
    "            if torch.is_tensor(lc):\n",
    "                lc = lc.cpu().numpy()\n",
    "\n",
    "            # make new masks relative to fraction of points we got to predict w/ \n",
    "            smask, rmask = my_utils.make_masks(lc, frac=frac)\n",
    "\n",
    "            # plug em in \n",
    "            lc[:,:,3:4] = smask[:,:, np.newaxis]\n",
    "            lc[:,:,4:5] = rmask[:,:, np.newaxis]\n",
    "            # CUDA~\n",
    "            lc = torch.tensor(lc)\n",
    "            lc = lc.to(device)\n",
    "            \n",
    "            subsampled_mask = lc[:,:,3:4]\n",
    "            seqlen = lc.size(0)\n",
    "            # \n",
    "            context_y = torch.cat((lc[:,:, 1:2] * subsampled_mask, subsampled_mask), -1)\n",
    "            # probabilities per batch?  \n",
    "\n",
    "            px, _ = net.get_reconstruction(lc[:,:, 0], context_y, lc[:,:, 0], num_samples=k_iwae)\n",
    "            pred_mean.append(px.mean.cpu().numpy()) # (10,batch_size, seq len,1)\n",
    "            # changing from logvar to std \n",
    "            pred_std.append(torch.exp(0.5 * px.logvar).cpu().numpy())\n",
    "\n",
    "            targets.append((lc[:,:, 1:2]).cpu().numpy())\n",
    "            masks.append(subsampled_mask.cpu().numpy())\n",
    "            tp.append(lc[:,:, 0].cpu().numpy())\n",
    "            \n",
    "    #             # how many batches of predictions for each frac?\n",
    "\n",
    "    # # 10 samples per light curve, 48 light curves, 621, 1\n",
    "    # # first 16 are frac[0], second 16 are frac[1], \n",
    "    pred_mean = np.concatenate(pred_mean, axis=1)\n",
    "    pred_std = np.concatenate(pred_std, axis=1)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    masks = np.concatenate(masks, axis=0)\n",
    "    tp = np.concatenate(tp, axis=0)\n",
    "    inputs = np.ma.masked_where(masks < 1., targets)\n",
    "    # inputs = np.ma.masked_where(masks < 1., targets)\n",
    "    print(f'pred_mean: {pred_mean.shape}', f'pred_std: {pred_std.shape}=', f'targets: {targets.shape}', f'masks: {masks.shape}', f'tps: {tp.shape}', f'inputs: {inputs.shape}')\n",
    "    # we're are sampling from the intermediate representation w/ k_iwae\n",
    "    # then we sample from the means/stds from the intermediate representation w/ k_iwae\n",
    "    \n",
    "    preds = np.random.randn(k_iwae // 2, k_iwae, pred_mean.shape[1], pred_mean.shape[2], pred_mean.shape[3]) * pred_std + pred_mean\n",
    "    preds = preds.reshape(-1, pred_mean.shape[1], pred_mean.shape[2], pred_mean.shape[3])\n",
    "    print(preds.shape)\n",
    "    median = preds.mean(0) #np.quantile(preds, 0.5, axis=0)\n",
    "    quantile2 = np.quantile(preds, 0.859, axis=0)\n",
    "    quantile1 = np.quantile(preds, 0.141, axis=0)\n",
    "    \n",
    "    w = 2.0\n",
    "    plt.figure(figsize=(12.5, 1.5))\n",
    "    for j in range(3):\n",
    "        plt.subplot(1, 3, j + 1)\n",
    "        plt.fill_between(tp[j], quantile1[j, :, 0], quantile2[j, :, 0], alpha=0.6, facecolor='#65c9f7', interpolate=True)\n",
    "        plt.plot(tp[j], median[j, :, 0], c='b', lw=w, label='Reconstructions')\n",
    "        #plt.plot(tp[n_max * j + index], gt[index], c='r', lw=w, label='Ground Truth')\n",
    "        plt.scatter(tp[j], inputs[j, :, 0], c='k', lw=w, label='Observed Data')\n",
    "    plt.show()\n",
    "\n",
    "viz_per_example(lcs[1], net, k_iwae=10, n_max=10, fracs=[0.05,0.1,0.15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
