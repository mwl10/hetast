{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c44f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# first make sure eval works\n",
    "## amount missing \n",
    "nonmiss = lcs.dataset[:,:,:,1].sum(axis=2) > 0\n",
    "mis2 = (nonmiss.sum(axis=1) == 1)\n",
    "mis1 = (nonmiss.sum(axis=1) == 2)\n",
    "mis0 = (nonmiss.sum(axis=1) == 0)\n",
    "\n",
    "misg = nonmiss[:,2]\n",
    "misr = nonmiss[:,0]\n",
    "misi = nonmiss[:,1]\n",
    "\n",
    "# get anomoly predictions\n",
    "\n",
    "\n",
    "# Subsampled Z more regular because thats what it sees during training?\n",
    "# pca3d = pca.fit_transform(zs_sub.reshape(zs_sub.shape[0],-1)) \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(pca3d[:,0],pca3d[:,1],pca3d[:,2])\n",
    "\n",
    "# pl = pca3d[:,0:2].T\n",
    "# k = gaussian_kde(pl, bw_method=0.1)(pl)\n",
    "# fig = plt.figure(figsize=(12,10))\n",
    "# ax = fig.add_subplot(111)\n",
    "# dens = ax.scatter(pca3d[:,0],pca3d[:,1],c=k, alpha=0.4,s=80)\n",
    "# fig.colorbar(dens,shrink=0.75)\n",
    "# plt.savefig('../img/pca2d')\n",
    "\n",
    "## don't look all that different\n",
    "#zs_sub = (np.random.randn(n_samples, qzs_sub.shape[0], qzs_sub.shape[2],qzs_sub.shape[3]) * qzs_sub[:,1,:,:] + qzs_sub[:,0,:,:]).mean(0)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_hetvae(\n",
    "    net,\n",
    "    dim,\n",
    "    dataloader,\n",
    "    frac=0.5,\n",
    "    k_iwae=1,\n",
    "    device='mps',\n",
    "    forecast=False,\n",
    "    qz_mean=False\n",
    "):\n",
    "    train_n = 0\n",
    "    train_loss,avg_loglik, mse, mae = 0, 0, 0,0\n",
    "    mean_mae, mean_mse = 0, 0\n",
    "    individual_nlls = []\n",
    "    indy_nlls = []\n",
    "    mses= []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # forecasting if this mask is set to first section of points only, not random selection\n",
    "            subsampled_mask = make_masks(batch, frac=frac, forecast=forecast)\n",
    "            ######################\n",
    "            errorbars = torch.swapaxes(batch[:,:,:,2], 2,1)\n",
    "            weights = errorbars.clone()\n",
    "            weights[weights!=0] = 1 / weights[weights!=0]\n",
    "            errorbars[errorbars!=0] = torch.log(errorbars[errorbars!=0])\n",
    "            logerr = errorbars.to(device)\n",
    "            weights = weights.to(device)\n",
    "            ######################\n",
    "            batch = batch.to(device)\n",
    "            subsampled_mask = subsampled_mask.to(device)\n",
    "            recon_mask = torch.logical_xor(subsampled_mask, batch[:,:,:,1])\n",
    "\n",
    "            context_y = torch.cat((\n",
    "              batch[:,:,:,1] * subsampled_mask, subsampled_mask\n",
    "            ), 1).transpose(2,1)\n",
    "            recon_context_y = torch.cat((\n",
    "              batch[:,:,:,1] * recon_mask, recon_mask\n",
    "            ), 1).transpose(2,1)\n",
    "            \n",
    "            loss_info = net.compute_unsupervised_loss(\n",
    "              batch[:, 0, :,0],\n",
    "              context_y,\n",
    "              batch[:, 0, :,0],\n",
    "              recon_context_y,\n",
    "              logerr,\n",
    "              weights,\n",
    "              num_samples=k_iwae,\n",
    "              qz_mean=qz_mean\n",
    "            )\n",
    "            \n",
    "            individual_nlls.append(loss_info.loglik_per_ex.cpu().numpy())\n",
    "            num_context_points = recon_mask.sum().item()\n",
    "            individual_nlls.append(loss_info.loglik_per_ex.cpu().numpy())\n",
    "            \n",
    "            train_loss += loss_info.composite_loss.item()* num_context_points\n",
    "            mse += loss_info.mse * num_context_points\n",
    "            mae += loss_info.mae * num_context_points\n",
    "            mean_mse += loss_info.mean_mse * num_context_points\n",
    "            mean_mae += loss_info.mean_mae * num_context_points\n",
    "            avg_loglik += loss_info.mogloglik * num_context_points\n",
    "            \n",
    "            mses.append(loss_info.mse)\n",
    "            indy_nlls.append(loss_info.mogloglik)\n",
    "            loss_info.mse\n",
    "            \n",
    "            train_n += num_context_points\n",
    "    print(\n",
    "        'nll: {:.4f}, mse: {:.4f}, mae: {:.4f}, '\n",
    "        'mean_mse: {:.4f}, mean_mae: {:.4f}'.format(\n",
    "            - avg_loglik / train_n,\n",
    "            mse / train_n,\n",
    "            mae / train_n,\n",
    "            mean_mse / train_n,\n",
    "            mean_mae / train_n\n",
    "        )\n",
    "    )\n",
    "    return -avg_loglik / train_n, mse / train_n, np.concatenate(individual_nlls, axis=1)[0], indy_nlls, mses\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## get qzs from folder, ensuring sure they're ordered the same as the loaded dataset\n",
    "# qzs = np.zeros((l,num_ref_points*latent_dim,2))\n",
    "# folder = 'qzs_all'\n",
    "# for obj in os.listdir(folder):\n",
    "#     qz = pd.read_csv(os.path.join(folder, obj,f'{obj}_qz.dat'), sep=' ',header=None).to_numpy()\n",
    "#     i = test.valid_files_df.index.get_loc(obj)\n",
    "#     qzs[i] = qz\n",
    "\n",
    "\n",
    "# def set_intrinsic_var(self):\n",
    "#     \"\"\"\n",
    "#     setting the normalized excess variance, as per the definition in Sánchez-Sáez et al. 2021\n",
    "#     \"\"\"\n",
    "#     intrinsic_vars = np.zeros((len(self.dataset),len(self.bands)))\n",
    "#     for i, object_lcs in enumerate(self.dataset):\n",
    "#         for j, lc in enumerate(object_lcs):\n",
    "#             dev_from_mean = (lc[:,1] - np.mean(lc[:,1]))\n",
    "#             avg_sq_dev_from_mean = np.matmul(dev_from_mean, dev_from_mean) * (1 / (len(lc) - 1))\n",
    "#             avg_sq_err = np.matmul(lc[:,2], lc[:,2]) / len(lc)\n",
    "#             intrinsic_var = avg_sq_dev_from_mean - avg_sq_err\n",
    "#             intrinsic_vars[i,j] = intrinsic_var\n",
    "#     self.intrinsic_var = intrinsic_vars\n",
    "\n",
    "        \n",
    "##  better software principles w/ dataset class,\n",
    "    # what does a dataset have? normalizing functions, \n",
    "\n",
    "# how we're gonna do synthetic data\n",
    "    ## do the convolution thing once we have one band's carma fit to get others, then manually add lags\n",
    "    \n",
    "## could test the effects of length / num points on anomaly detection w/ syntehtic data, anomalous fits\n",
    "# for dataset balancing, its probably more about number of observed points\n",
    "# for training, its more about light cures being in the same time ranges \n",
    "\n",
    "\n",
    "## 12/27\n",
    "    ## mcmc sample errors on the fly during training, error bar code \n",
    "\n",
    "# weird decisions to make (for talkinig purposes)\n",
    "    # normlaize across training or individually? \n",
    "    # how to 'chop' by lenght or num points, min points?\n",
    "    # injecting domain-specific knowledge\n",
    "        # >1 mag err, keep obejcts between 13-20 mag, objects w/ certain intrinsitc var\n",
    "    # formatting, setting masks\n",
    "    \n",
    "    # hyper-parameter things\n",
    "    \n",
    "    # how to use error bars on input data? \n",
    "        \n",
    "        \n",
    "    # lr scheduling? \n",
    "    # visualize kl annealing? wait_until_kl_inc is a hyperparam! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec810561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from random import SystemRandom\n",
    "from argparse import Namespace\n",
    "from model import load_network\n",
    "import utils\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "warnings.simplefilter('ignore', np.RankWarning) # set warning for polynomial fitting\n",
    "LCS = utils.get_data('../../datasets/ZTF_g_test', start_col=1)\n",
    "\n",
    "### trials is first command line arg, niters per trial is second  ####\n",
    "\n",
    "def define_model_args(trial):\n",
    "    args = Namespace(\n",
    "        niters=int(sys.argv[2]),  ### num iters per trial \n",
    "        enc_num_heads=trial.suggest_categorical(\"enc_num_heads\",[1,2,4,8,16,32]),\n",
    "        embed_time = trial.suggest_categorical(\"embed_time\",[32,64,128,256]),\n",
    "        width=trial.suggest_categorical('width', [128,256,512,1024]),\n",
    "        num_ref_points=trial.suggest_categorical('num_ref_points', [8,16,24,32,48]),\n",
    "        rec_hidden=trial.suggest_categorical(\"rec_hidden\",[32,64,128,256]),\n",
    "        latent_dim= trial.suggest_categorical(\"latent_dim\",[32,64,128,256]),\n",
    "        dropout =0.0,#trial.suggest_float(\"dropout\", 0.0,0.5,step=0.1),\n",
    "        lr=0.0003,\n",
    "        frac = 0.5,\n",
    "        mixing='concat',\n",
    "        mse_weight=5,#trial.suggest_int(\"mse_weight\",1,20),\n",
    "        data_folder = 'ZTF_gband_test',\n",
    "        batch_size = 2,\n",
    "        patience = 10000,\n",
    "        scheduler = False,\n",
    "        warmup = 4000,#trial.suggest_int('warmup', 3000,5000),\n",
    "        k_iwae=1,\n",
    "        kl_annealing=True,\n",
    "        kl_zero=False, \n",
    "        net='hetvae', \n",
    "        norm=True, \n",
    "        save=False, \n",
    "        seed=0, \n",
    "        std=0.1, \n",
    "        device='mps',\n",
    "        checkpoint = '',\n",
    "        save_at = 10000000, \n",
    "        inc_errors = False,\n",
    "        nodet=False,\n",
    "        print_at=1\n",
    "    ) \n",
    "    return args\n",
    "\n",
    "\n",
    "def train(trial, args, lcs):\n",
    "    experiment_id = int(SystemRandom().random() * 10000000)\n",
    "    #print(args, experiment_id)\n",
    "    ##################################\n",
    "    seed = args.seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    ##################################\n",
    "    device = torch.device(args.device)\n",
    "    \n",
    "    data_obj = lcs.data_obj\n",
    "    N_union_tp = trial.suggest_categorical('N_union_tp', [500,1000,1500,2000,2500,3000,3500,4000,4500,5000])\n",
    "    lcs.set_union_tp(uniform=True, n=N_union_tp)\n",
    "    train_loader = data_obj[\"train_loader\"]\n",
    "    test_loader = data_obj[\"test_loader\"]\n",
    "    val_loader = data_obj[\"valid_loader\"]\n",
    "    dim = data_obj[\"input_dim\"]\n",
    "    union_tp = torch.tensor(lcs.union_tp)\n",
    "    \n",
    "    net = load_network(args, dim, union_tp)\n",
    "    params = list(net.parameters())\n",
    "    optimizer = optim.Adam(params, lr=args.lr)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    epoch = 1\n",
    "    loss = 1000000\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    lrs = []\n",
    "    \n",
    "    model_size = utils.count_parameters(net) \n",
    "    print(f'{model_size=}')\n",
    "    ############### have patience ##########\n",
    "    best_loss = loss\n",
    "    patience_counter = 0\n",
    "    ######################################## \n",
    "    if args.kl_annealing:\n",
    "        kl_coefs = utils.frange_cycle_linear(6000, n_cycle=16)\n",
    "    ##################\n",
    "    for itr in range(epoch, epoch+args.niters):\n",
    "        print(f'{itr},', end='', flush=True)\n",
    "    \n",
    "        train_loss = 0\n",
    "        train_n = 0\n",
    "        avg_loglik, avg_wloglik, avg_kl, mse, wmse, mae = 0, 0, 0, 0, 0, 0 \n",
    "        if args.kl_annealing:\n",
    "            kl_coef = kl_coefs[itr] # need global number of epochs to continue on based on schedule\n",
    "        elif args.kl_zero:\n",
    "            kl_coef = 0\n",
    "        else:\n",
    "            kl_coef = 1\n",
    "        ###################################################################  \n",
    "        for train_batch in train_loader:\n",
    "            batch_len = train_batch.shape[0]\n",
    "            \n",
    "            ### including obs error ##############################\n",
    "            errorbars = torch.swapaxes(train_batch[:,:,:,2], 2,1)\n",
    "            weights = errorbars.clone()\n",
    "            weights[weights!=0] = 1 / weights[weights!=0]\n",
    "            errorbars[errorbars!=0] = torch.log(errorbars[errorbars!=0]**2)\n",
    "            logerr = errorbars.to(device) # log variance on observations \n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            ############################################################\n",
    "            subsampled_mask = utils.make_masks(train_batch, frac=args.frac)\n",
    "            train_batch = train_batch.to(device)\n",
    "            subsampled_mask = subsampled_mask.to(device)\n",
    "            recon_mask = torch.logical_xor(subsampled_mask, train_batch[:,:,:,1])\n",
    "            context_y = torch.cat((\n",
    "              train_batch[:,:,:,1] * subsampled_mask, subsampled_mask), 1).transpose(2,1)\n",
    "            recon_context_y = torch.cat((\n",
    "                train_batch[:,:,:,1] * recon_mask, recon_mask), 1).transpose(2,1)\n",
    "            #############################################################\n",
    "            loss_info = net.compute_unsupervised_loss(\n",
    "                train_batch[:, 0, :,0],\n",
    "                context_y,\n",
    "                train_batch[:, 0, :,0],\n",
    "                recon_context_y,\n",
    "                logerr,\n",
    "                weights,\n",
    "                num_samples=args.k_iwae,\n",
    "                beta=kl_coef,\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if args.inc_errors:\n",
    "                loss_info.weighted_comp_loss.backward()\n",
    "            else:\n",
    "                loss_info.composite_loss.backward()\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            ########### train loss ##################################\n",
    "            train_loss += loss_info.composite_loss.item() * batch_len\n",
    "            avg_loglik += loss_info.loglik * batch_len\n",
    "            avg_wloglik += loss_info.wloglik * batch_len\n",
    "            avg_kl += loss_info.kl * batch_len\n",
    "            mse += loss_info.mse * batch_len\n",
    "            wmse += loss_info.wmse * batch_len\n",
    "            mae += loss_info.mae * batch_len\n",
    "            train_n += batch_len\n",
    "            #########################################################\n",
    "        \n",
    "    \n",
    "        ####### nan, stop training #############\n",
    "        if np.isnan(train_loss / train_n):\n",
    "            print('nan in loss,,,,,,,,, stopping')\n",
    "            break\n",
    "        \n",
    "        #### validation loss\n",
    "        val_nll, val_mse, val_indiv_nlls = utils.evaluate_hetvae(\n",
    "            net,\n",
    "            dim,\n",
    "            val_loader, \n",
    "            0.5,\n",
    "            device=args.device\n",
    "        )\n",
    "        \n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        if scheduler:\n",
    "            scheduler.step(val_nll) \n",
    "        \n",
    "        ######## print train / valid losses ########\n",
    "        if itr % args.print_at == 0:\n",
    "            print(\n",
    "                '\\tIter: {}, train loss: {:.4f}, avg nll: {:.4f}, avg wnll: {:.4f}, avg kl: {:.4f}, '\n",
    "                'mse: {:.6f}, wmse: {:.6f}, mae: {:.6f}, val nll: {:.4f}, val mse {:.4f}'.format(\n",
    "                    itr,\n",
    "                    train_loss / train_n,\n",
    "                    -avg_loglik / train_n,\n",
    "                    -avg_wloglik / train_n,\n",
    "                    avg_kl / train_n,\n",
    "                    mse / train_n,\n",
    "                    wmse / train_n,\n",
    "                    mae / train_n,\n",
    "                    val_nll,\n",
    "                    val_mse))\n",
    "                \n",
    "        ####### test loss every 10 itrs, save losses too #############\n",
    "        if itr % 10 == 0:\n",
    "#             test_nll, test_mse, indiv_nlls = utils.evaluate_hetvae(\n",
    "#                 net,\n",
    "#                 dim,\n",
    "#                 test_loader, \n",
    "#                 0.5,\n",
    "#                 device=args.device\n",
    "#                 )\n",
    "            \n",
    "            train_losses.append([(-avg_loglik / train_n).item(),(mse / train_n).item(),(avg_kl / train_n).item()])\n",
    "#             test_losses.append([test_nll.item(),test_mse.item()])\n",
    "            val_losses.append([val_nll.item(),val_mse.item()])\n",
    "#             print('test nll: {:.4f}, test mse: {:.4f}'.format(test_nll,test_mse))\n",
    "            \n",
    "\n",
    "        ###### optuna stuff #######################        \n",
    "        trial.report(val_nll, itr)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return val_nll\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    args = define_model_args(trial)\n",
    "    loss = train(trial,args, LCS)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=int(sys.argv[1]), timeout=10000)\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85473645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "#### plotting forecasts, and getting forecast losses\n",
    "\n",
    "for i, obj_folder in enumerate(glob.glob('./datasets/obj_dirs/*')):\n",
    "    obj_name = obj_folder.split('/')[-1].split('_')[1]\n",
    "    print(f'{i},', end='', flush=True)\n",
    "    print(obj_folder)\n",
    "    \n",
    "    try:\n",
    "        lcs = utils.get_data(obj_folder, sep=',', start_col=1, batch_size=1, min_length=40, \\\n",
    "                             n_union_tp=3500, num_resamples=111, shuffle=False, chop=False)\n",
    "        train = lcs.data_obj['train_loader']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    net, optimizer, args, epoch, loss = utils.load_checkpoint('./ZTFgr_small0.7880523800849915.h5', lcs.data_obj, device='cuda')\n",
    "    examples, z, recons = utils.predict(train, net, device='cuda', subsample=False, target_x=None, forecast=True)\n",
    "    utils.save_recon(examples, recons,z,obj_name, bands=lcs.bands, save_folder='forecast_plots')\n",
    "\n",
    "    train_nll, mse, individual_nlls = utils.evaluate_hetvae(\n",
    "                net,\n",
    "                2,\n",
    "                train,\n",
    "                0.5,\n",
    "                k_iwae=2,\n",
    "                device='cuda',\n",
    "                forecast=True\n",
    "                )\n",
    "\n",
    "    with open('forecast_losses.txt', 'a') as f:\n",
    "        f.write(f'{obj_name},{train_nll},{mse}\\n')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbde0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "#### plotting forecasts, and getting forecast losses\n",
    "\n",
    "for i, obj_folder in enumerate(glob.glob('./datasets/obj_dirs/*')):\n",
    "    obj_name = obj_folder.split('/')[-1].split('_')[1]\n",
    "    print(f'{i},', end='', flush=True)\n",
    "    print(obj_folder)\n",
    "    \n",
    "    try:\n",
    "        lcs = utils.get_data(obj_folder, sep=',', start_col=1, batch_size=1, min_length=40,\\\n",
    "                             n_union_tp=3500, num_resamples=111, shuffle=False, chop=True, extend=5000)\n",
    "        \n",
    "        train = lcs.data_obj['train_loader']\n",
    "        lcs.set_target_x(5001)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    net, optimizer, args, epoch, loss = utils.load_checkpoint('./ZTFgr_small0.7880523800849915.h5', \\\n",
    "                                                              lcs.data_obj, device='cuda')\n",
    "    \n",
    "    examples, z, recons = utils.predict(train, net, device='cuda', subsample=False, \\\n",
    "                                        target_x=lcs.target_x, forecast=False)\n",
    "    \n",
    "    utils.save_recon(examples, recons,z,obj_name, bands=lcs.bands, save_folder='interpolate_plots')\n",
    "\n",
    "\n",
    "    train_nll, mse, individual_nlls = utils.evaluate_hetvae(\n",
    "                net,\n",
    "                2,\n",
    "                train,\n",
    "                0.5,\n",
    "                k_iwae=2,\n",
    "                device='cuda',\n",
    "                forecast=False\n",
    "                )\n",
    "\n",
    "    with open('losses.txt', 'a') as f:\n",
    "        f.write(f'{obj_name},{train_nll},{mse}\\n')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "#index = int(sys.argv[1])\n",
    "lcs = os.listdir('./ZTFgr_small/g/')\n",
    "obj_names = [lc.split('_')[0] for lc in lcs]\n",
    "print(len(obj_names))\n",
    "for obj_name in obj_names:\n",
    "    dir_name = os.path.join('obj_dirs', f'ZTFgr_{obj_name}')\n",
    "    os.mkdir(dir_name)\n",
    "    g = f'{dir_name}/g'\n",
    "    r = f'{dir_name}/r'\n",
    "    os.mkdir(g)\n",
    "    os.mkdir(r)\n",
    "    g_file = glob.glob(f'ZTFgr_small/g/{obj_name}*')[0]\n",
    "    r_file = glob.glob(f'ZTFgr_small/r/{obj_name}*')[0]\n",
    "    print(dir_name)\n",
    "    shutil.copy(g_file,g)\n",
    "    shutil.copy(r_file,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH -p res-gpu-large\n",
    "#SBATCH --qos=long-high-prio\n",
    "#SBATCH --job-name=hetvae\n",
    "#SBATCH --output=\"test_loss-%j.out\"\n",
    "#SBATCH -t 10:00:00\n",
    "\n",
    "source /etc/profile\n",
    "source activate $HOME/miniconda/envs/hetvae\n",
    "\n",
    "python3 $HOME/astr/hetast/src/forecast_script.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c3fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6f7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315bbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a67abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6a660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cc727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def decode(net,qzs,disc_path,target_x, n_samples=10, device='mps', batch_size=1):\n",
    "    \n",
    "#     #input qzs shape (n_examples, 2, 16, 64)\n",
    "#     #input disc path shape (268, 16, 64)\n",
    "    \n",
    "#     print(qzs.shape)\n",
    "#     print(disc_path.shape)\n",
    "    \n",
    "#     z = (np.random.randn(n_samples, qzs.shape[0],qzs.shape[2], qzs.shape[3]) \\\n",
    "#          * qzs[:,0,:,:] + qzs[:,1,:,:]) # sample z\n",
    "#     z = torch.from_numpy(z)\n",
    "    \n",
    "#     disc_path = torch.from_numpy(disc_path)\n",
    "#     disc_path = disc_path.unsqueeze(0).repeat_interleave(n_samples,dim=0)\n",
    "    \n",
    "#     z = torch.cat((z, disc_path), -1) # 10,234,16,128, \n",
    "#     z = torch.swapaxes(z,0,1)\n",
    "#     z = z.type(torch.Tensor)\n",
    "    \n",
    "#     pred_mean, pred_std = [], []\n",
    "#     target_tp = []\n",
    "#     dl = torch.utils.data.DataLoader(z, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(dl):\n",
    "#             batch = batch.to(device)\n",
    "#             tx = torch.tensor(target_x[i*batch_size:(i*batch_size + batch_size)])[:,0]\n",
    "#             tx = tx.to(device)\n",
    "#             # zbatch shape should be 10,1,16,128\n",
    "#             px = net.decode(batch.swapaxes(0,1), tx) \n",
    "#             pred_mean.append(px.mean.cpu().numpy())\n",
    "#             pred_std.append(torch.exp(0.5 * px.logvar).cpu().numpy())\n",
    "#             target_tp.append(tx.cpu().numpy())\n",
    "\n",
    "#     pred_mean = np.concatenate(pred_mean, axis=1)\n",
    "#     pred_std = np.concatenate(pred_std, axis=1)\n",
    "#     target_tp = np.concatenate(target_tp, axis=0)\n",
    "#     return pred_mean,pred_std,target_tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_recons(recon_info, obj_names, bands=['r','i','g'], save_folder='reconstructions'):\n",
    "    examples = recon_info['examples']\n",
    "    recons = recon_info['recons']\n",
    "    zs = recon_info['zs']\n",
    "    target_tp = recons['target_tp']\n",
    "    pred_mean = recons['pred_mean']\n",
    "    pred_std = recons['pred_std']\n",
    "    target = examples['target']\n",
    "    inputs = examples['inputs']\n",
    "    tp = examples['tp']\n",
    "    if not os.path.isdir(save_folder): os.mkdir(save_folder)     \n",
    "    for i in range(len(inputs)):\n",
    "        obj_folder = os.path.join(save_folder, obj_names[i])\n",
    "        if not os.path.isdir(obj_folder): os.mkdir(obj_folder)\n",
    "        pred_t = target_tp[i].nonzero()[0] # same for all bands? \n",
    "        t = target_tp[:1,pred_t]\n",
    "        for j,band in enumerate(bands):\n",
    "            mean_mag = pred_mean[i,pred_t,j][np.newaxis]\n",
    "            mag_std = pred_std[i,pred_t,j][np.newaxis]\n",
    "            lc = np.concatenate((t, mean_mag, mag_std), axis=0).T\n",
    "            save_file = os.path.join(obj_folder,f'{obj_names[i]}_{band}.dat')\n",
    "            np.savetxt(save_file, lc)\n",
    "            print(f'{lc.shape=} and {band=} saved to {save_file}')\n",
    "        z_save_file = os.path.join(save_folder,obj_names[i],f'{obj_names[i]}_qz.dat')\n",
    "        zmu = zs['qz_mean'][i].flatten()\n",
    "        zstd = zs['qz_std'][i].flatten()\n",
    "        z = np.concatenate((zmu[np.newaxis], zstd[np.newaxis]), axis=0).T\n",
    "        np.savetxt(z_save_file,z)\n",
    "        print(f'{z.shape=} for {obj_names[i]=} saved to {z_save_file}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_qzs(qzs, obj_names, bands=['r','i','g'], save_folder='qzs'):\n",
    "    if not os.path.isdir(save_folder): os.mkdir(save_folder)     \n",
    "    for i in range(len(qzs)):\n",
    "        obj_folder = os.path.join(save_folder, obj_names[i])\n",
    "        if not os.path.isdir(obj_folder): os.mkdir(obj_folder)\n",
    "        qz_save_file = os.path.join(save_folder,obj_names[i],f'{obj_names[i]}_qz.dat')\n",
    "        np.savetxt(qz_save_file,qzs[i])\n",
    "        print(f'{qzs[i].shape=} for {obj_names[i]=} saved to {qz_save_file}') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e93355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intra-example formatting:\\n')\n",
    "\n",
    "print('light curve g\\n')\n",
    "print('  t    mag  magerr')\n",
    "print(np.array([[0,4.3,0.09],\n",
    "        [1.2,5.2,0.08]]))\n",
    "print('\\n')\n",
    "print('light curve r\\n')\n",
    "print(np.array([[0,3.2,0.12],\n",
    "        [2.33,4.1,0.03]]))\n",
    "print('\\n')\n",
    "print('becomes:\\n')\n",
    "print('light curve g\\n')\n",
    "print(np.array([[0,4.3,0.09],\n",
    "        [1.2,5.2,0.08],\n",
    "        [2.33,0,0]]))\n",
    "print('\\n')\n",
    "print('light curve r\\n')\n",
    "print(np.array([[0,3.2,0.12],\n",
    "                [1.2,0.0,0.00],[2.33,4.1,0.03]]))\n",
    "print('\\nif 5 is the length of the longest \\nintra-example union-time array, becomes:\\n')\n",
    "print('light curve g\\n')\n",
    "print(np.array([[0,4.3,0.09],\n",
    "        [1.2,5.2,0.08],\n",
    "        [2.33,0,0],[0,0,0],[0,0,0]]))\n",
    "print('\\n')\n",
    "print('light curve r\\n')\n",
    "print(np.array([[0,3.2,0.12],\n",
    "                [1.2,0.0,0.00],[2.33,4.1,0.03],[0,0,0],[0,0,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7504588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_synth_data(base_folder='/Users/mattlowery/Desktop/code/astro/hetast/src/test_data', save_folder='synth_test_data', seed = 0, kernel='drw',duration=730, n=180, uniform=False):\n",
    "    \"\"\"\n",
    "    This function creates and loads a synthetic dataset relative to a given kernel (drw or dho), \n",
    "    distributing the kernel params relative to a real dataset  \n",
    "    \n",
    "    parameters:\n",
    "        folder             (str)      --> synthetic data will be fit to the light curves in this folder to get an honest distribution of params\n",
    "         ----- optional -----\n",
    "        seed               (int)      --> random seed, this matters to keep the shuffles consistent\n",
    "        batch_size         (int)      --> for the network, usually a multiple of 2\n",
    "        kernel             (str)      --> 'drw' (dampled random walk), a carma(1) process; 'dho' (damped harmonic oscillator), a carma(2,1) process\n",
    "        n\n",
    "        duration \n",
    "        \n",
    "    drw_kernel params --> 'tau' is decorelation timescale, 'amp' is the amplitude\n",
    "    dho_kernel params --> a1 = 2 * Xi * w0\n",
    "                       a2 = w0 ** 2\n",
    "                       b0 = sigma\n",
    "                       b1 = tau * b0\n",
    "                       wherein Xi is the damping ratio, w0 is the natural oscillation frequency, sigma is the amplitude\n",
    "                       of the short term perturbing white noise, tau is the characteristic timescale of the perturbation process\n",
    "    returns:\n",
    "        a dictionary of torch dataloaders with data formatted as necessary for network training \n",
    "        , as well as the dimension and union of all the time points\n",
    "    \"\"\"\n",
    "    synth_band_name = 'simband1'\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    if not os.path.isdir(base_folder):\n",
    "        raise Exception(f\"{base_folder} is not a directory\")\n",
    "        \n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "        os.mkdir(os.path.join(save_folder,synth_band_name))   \n",
    "        \n",
    "    lcs = DataSet(name=base_folder, min_length=50, sep=',', start_col=1)\n",
    "    band_folders = os.listdir(base_folder)\n",
    "    for band_folder in band_folders:\n",
    "        band = band_folder.lower()\n",
    "        lcs.add_band(band, os.path.join(base_folder, band_folder))\n",
    "        \n",
    "    lcs.filter()         \n",
    "    lcs.prune_outliers()\n",
    "    lcs.set_carma_fits(kernel=kernel)\n",
    "    lcs.set_snr()\n",
    "    \n",
    "    synth_lcs = []\n",
    "    for i, params in enumerate(lcs.carma_fits):\n",
    "        if np.isnan(params[0]):\n",
    "            continue\n",
    "        if kernel == 'drw':\n",
    "            term = DRW_term(*np.log(params))\n",
    "        else:\n",
    "            term = DHO_term(*np.log(params))\n",
    "                \n",
    "        # kernel, snr, duration (days), n      \n",
    "        if uniform == True:\n",
    "            lc = np.array(gpSimFull(term,lcs.snr[i,0], duration, n)).transpose(1,0)\n",
    "        else: \n",
    "            lc = np.array(gpSimRand(term,lcs.snr[i,0], duration, n)).transpose(1,0)   \n",
    "        fpath = f'{os.path.join(save_folder, synth_band_name,str(i))}_.csv'\n",
    "        np.savetxt(fpath,lc, delimiter=',', header=' , , ', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96322e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_carma_fits(self, kernel='drw'):\n",
    "        carma_fits = []\n",
    "        for i, object_lcs in enumerate(self.dataset):\n",
    "            print(i, end='')\n",
    "            lc = object_lcs[0]\n",
    "            if kernel == 'drw':\n",
    "                fit = drw_fit(lc[:,0], lc[:,1], lc[:,2])\n",
    "            else:\n",
    "                fit = dho_fit(lc[:,0], lc[:,1], lc[:,2])   \n",
    "            carma_fits.append(fit)          \n",
    "        self.carma_fits = carma_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aea55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ranges = [np.ptp(lc[:,0]) for object_lcs in self.dataset for lc in object_lcs]\n",
    "        bins = np.arange(max(ranges), step=50)\n",
    "        range1 = [np.ptp(object_lcs[0][:,0]) for object_lcs in self.dataset]\n",
    "        range2 = [np.ptp(object_lcs[1][:,0]) for object_lcs in self.dataset]\n",
    "        range3 = [np.ptp(object_lcs[2][:,0]) for object_lcs in self.dataset]\n",
    "        plt.hist(range1,alpha=0.45,label='r',bins=bins)\n",
    "        plt.hist(range3,alpha=0.35,label='g',bins=bins)\n",
    "        plt.hist(range2,alpha=0.45,label='i',bins=bins)\n",
    "        \n",
    "        plt.title('Distribution of Light Curve Ranges')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.xlabel('range (days)')\n",
    "        plt.savefig('ranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c217ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source ~/miniconda/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2009c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5dc09839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #might need an interpolation turn off (just predict based on ), look at predict function one more time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hetvae",
   "language": "python",
   "name": "hetvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
